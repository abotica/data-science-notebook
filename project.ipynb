{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for Data Science Course\n",
    "\n",
    "### Made by: Andrija Botica, Daria Milić, Karlo Nevešćanin\n",
    "### Professor: dr. sc. Toni Perković"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Our idea was to use web scraping to gather data from Croatian stores.\n",
    "\n",
    "The data can then be used to find the product you want for the cheapest price.\n",
    "\n",
    "Data will also be used in Human Computer Interaction course where we will implement full-stack web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "We used Python, BeautifulSoup and Selenium to scrape data from Croatian stores.\n",
    "\n",
    "We scraped data from the following stores:\n",
    "- Konzum - Webshop\n",
    "- Ribola - Wolt\n",
    "- Studenac - Wolt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start scraping we need to install the following libraries:\n",
    "- requests\n",
    "- bs4\n",
    "- lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install requests bs4 lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to include them in our code (lxml does not need to be included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konzum webshop has different categories of products, so we need to scrape them separately.\n",
    "\n",
    "Every category has its own URL, so we need to scrape them one by one.\n",
    "\n",
    "We define a variable `categories` which contains all the categories we want to scrape.\n",
    "To get each category we will create `categories_links` and `categories_urls` lists.\n",
    "\n",
    "The latter will have base URL appended to each category link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konzum webshop\n",
    "URL = \"https://www.konzum.hr\"\n",
    "\n",
    "# Variables\n",
    "categories = []\n",
    "categories_links = []\n",
    "categories_urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scrape the data from Konzum webshop and extract the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.content, \"lxml\")\n",
    "table = soup.find(\"section\", attrs={\"class\": \"py-3\"})\n",
    "categories = table.find_all(\"a\", attrs={\"class\": \"category-box__link\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create each categories URL using `for loops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    categories_links.append(category['href'])\n",
    "\n",
    "for links in categories_links:\n",
    "    categories_urls.append(URL + links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep count of the number of products we will use `total_products` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_products = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scrape the data from the categories.\n",
    "\n",
    "For each category url from `categories_urls` we will scrape the data and extract the product name, price and image alongside with name of the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_url in categories_urls:\n",
    "            page = requests.get(f'{category_url}').text\n",
    "            pageSoup = BeautifulSoup(page, 'lxml')\n",
    "            subCategories = pageSoup.find('ul', class_='plain-list mb-3')\n",
    "            subCategories_aTag = subCategories.find_all('a')\n",
    "\n",
    "            if subCategories_aTag:\n",
    "                for aTag in subCategories_aTag:\n",
    "                    subCategoryLink = aTag.get('href')\n",
    "                    subCategoryURL = URL + subCategoryLink\n",
    "\n",
    "                    if subCategoryURL:\n",
    "                        page_number = 1\n",
    "                        while True:\n",
    "                            finalPage = requests.get(f'{subCategoryURL}?page={page_number}')\n",
    "                            if finalPage.status_code != 200:\n",
    "                                break\n",
    "                            finalSoup = BeautifulSoup(finalPage.text, 'lxml')\n",
    "                            allItems = finalSoup.find('div', class_='col-12 col-md-12 col-lg-10')\n",
    "                            if not allItems:\n",
    "                                break\n",
    "\n",
    "                            productsList = allItems.find('div', class_='product-list product-list--md-5 js-product-layout-container product-list--grid')\n",
    "                            if not productsList:\n",
    "                                break\n",
    "\n",
    "                            articles = productsList.find_all('article', class_='product-item product-default')\n",
    "                            if not articles:\n",
    "                                break\n",
    "\n",
    "                            for article in articles:\n",
    "                                articleImageURL = article.find('img').get('src')\n",
    "                                if articleImageURL:\n",
    "                                    articleTittleTag = article.find('h4', class_='product-default__title')\n",
    "                                    if articleTittleTag:\n",
    "                                        articleNameTag = articleTittleTag.find('a', class_='link-to-product')\n",
    "                                        if articleNameTag:\n",
    "                                            articleName = articleNameTag.get_text(strip=True)\n",
    "                                            if articleName:\n",
    "                                                articleEuro = article.find('span', class_='price--kn').text\n",
    "                                                articleCent = article.find('span', class_='price--li').text\n",
    "                                                total_products += 1\n",
    "\n",
    "                                                print(f'Name: {articleName}', f'Price: {articleEuro}.{articleCent}', f'Image URL: {articleImageURL}', f'Store: Konzum')\n",
    "                            page_number += 1\n",
    "\n",
    "print(f'Total number of products: {total_products}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created this code by looking at HTML code of the website. We simply tell BeatifulSoup to find the tags that contain the data we need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
