{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for Data Science Course\n",
    "\n",
    "### Made by: Andrija Botica, Daria Milić, Karlo Nevešćanin\n",
    "### Professor: dr. sc. Toni Perković"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Our idea was to use web scraping to gather data from Croatian stores.\n",
    "\n",
    "The data can then be used to find the product you want for the cheapest price.\n",
    "\n",
    "Data will also be used in Human Computer Interaction course where we will implement full-stack web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "We used Python, BeautifulSoup and Selenium to scrape data from Croatian stores.\n",
    "\n",
    "We scraped data from the following stores:\n",
    "- Konzum - Webshop\n",
    "- Ribola - Wolt\n",
    "- Studenac - Wolt\n",
    "- Tommy - Wolt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start scraping we need to install the following libraries:\n",
    "- requests\n",
    "- bs4\n",
    "- lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install requests bs4 lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to include them in our code (lxml does not need to be included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konzum webshop has different categories of products, so we need to scrape them separately.\n",
    "\n",
    "Every category has its own URL, so we need to scrape them one by one.\n",
    "\n",
    "We define a variable `categories` which contains all the categories we want to scrape.\n",
    "To get each category we will create `categories_links` and `categories_urls` lists.\n",
    "\n",
    "The latter will have base URL appended to each category link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konzum webshop\n",
    "URL = \"https://www.konzum.hr\"\n",
    "\n",
    "# Variables\n",
    "categories = []\n",
    "categories_links = []\n",
    "categories_urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scrape the data from Konzum webshop and extract the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.content, \"lxml\")\n",
    "table = soup.find(\"section\", attrs={\"class\": \"py-3\"})\n",
    "categories = table.find_all(\"a\", attrs={\"class\": \"category-box__link\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create each categories URL using `for loops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    categories_links.append(category['href'])\n",
    "\n",
    "for links in categories_links:\n",
    "    categories_urls.append(URL + links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep count of the number of products we will use `total_products` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_products = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scrape the data from the categories.\n",
    "\n",
    "For each category url from `categories_urls` we will scrape the data and extract the product name, price and image alongside with name of the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_url in categories_urls:\n",
    "            page = requests.get(f'{category_url}').text\n",
    "            pageSoup = BeautifulSoup(page, 'lxml')\n",
    "            subCategories = pageSoup.find('ul', class_='plain-list mb-3')\n",
    "            subCategories_aTag = subCategories.find_all('a')\n",
    "\n",
    "            if subCategories_aTag:\n",
    "                for aTag in subCategories_aTag:\n",
    "                    subCategoryLink = aTag.get('href')\n",
    "                    subCategoryURL = URL + subCategoryLink\n",
    "\n",
    "                    if subCategoryURL:\n",
    "                        page_number = 1\n",
    "                        while True:\n",
    "                            finalPage = requests.get(f'{subCategoryURL}?page={page_number}')\n",
    "                            if finalPage.status_code != 200:\n",
    "                                break\n",
    "                            finalSoup = BeautifulSoup(finalPage.text, 'lxml')\n",
    "                            allItems = finalSoup.find('div', class_='col-12 col-md-12 col-lg-10')\n",
    "                            if not allItems:\n",
    "                                break\n",
    "\n",
    "                            productsList = allItems.find('div', class_='product-list product-list--md-5 js-product-layout-container product-list--grid')\n",
    "                            if not productsList:\n",
    "                                break\n",
    "\n",
    "                            articles = productsList.find_all('article', class_='product-item product-default')\n",
    "                            if not articles:\n",
    "                                break\n",
    "\n",
    "                            for article in articles:\n",
    "                                articleImageURL = article.find('img').get('src')\n",
    "                                if articleImageURL:\n",
    "                                    articleTittleTag = article.find('h4', class_='product-default__title')\n",
    "                                    if articleTittleTag:\n",
    "                                        articleNameTag = articleTittleTag.find('a', class_='link-to-product')\n",
    "                                        if articleNameTag:\n",
    "                                            articleName = articleNameTag.get_text(strip=True)\n",
    "                                            if articleName:\n",
    "                                                articleEuro = article.find('span', class_='price--kn').text\n",
    "                                                articleCent = article.find('span', class_='price--li').text\n",
    "                                                total_products += 1\n",
    "\n",
    "                                                print(f'Name: {articleName}', f'Price: {articleEuro}.{articleCent}', f'Image URL: {articleImageURL}', f'Store: Konzum')\n",
    "                            page_number += 1\n",
    "\n",
    "print(f'Total number of products: {total_products}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created this code by looking at HTML code of the website. We simply tell BeatifulSoup to find the tags that contain the data we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just one scraping example. There were other stores that we scraped data from, some of which required use of `Selenium` because of their dynamic content.\n",
    "\n",
    "Following stores required `Selenium` for scraping due to their dynamic content:\n",
    "- Ribola - Wolt\n",
    "- Studenac - Wolt\n",
    "- Tommy - Wolt\n",
    "\n",
    "\n",
    "Our scraping code can be found on this link: https://github.com/abotica/data-science-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start scraping we need to install the following libraries:\n",
    "- selenium (used for scraping)\n",
    "- pandas (used for creating dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install selenium pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "Now we need to include them in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will be scraping **Studenac - Wolt**\n",
    "\n",
    "We will start by opening the Wolt Studenac page using Selenium and handle any pop-ups that may appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WOLT_URL = \"https://wolt.com/hr/hrv/split/venue/studenac-kralja-zvonimira-t300/items/\"\n",
    "URL = \"https://wolt.com/hr/hrv/split/venue/studenac-kralja-zvonimira-t300\"\n",
    "Store_name = \"Wolt-Studenac\"\n",
    "categories_links = []\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(URL)\n",
    "\n",
    "try:\n",
    "    consent_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.deorxlo button.cbc_TextButton_rootCss_7cfd4\"))\n",
    "    )\n",
    "    consent_button.click()\n",
    "    time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(f\"Consent button not found or not clickable: {e}\")\n",
    "\n",
    "try:\n",
    "    x_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[class='cbc_IconButton_root_7cfd4 c1hmjr97']\"))\n",
    "    )\n",
    "    x_button.click()\n",
    "    time.sleep(1) \n",
    "except Exception as e:\n",
    "    print(f\"X button not found or not clickable: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like above, each store has different categories of products, and almost every category has a few subcategories.\n",
    "\n",
    "Every subcategory has its own URL, so we need to scrape them one by one.\n",
    "\n",
    "The following codes show how to get each subcategory's URL, which will then be forwarded to the `scrape_products()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we extract the URL of each category on the page and store it in `categories_links`.\n",
    "\n",
    "Next, we filter these links to remove any irrelevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subMenu = driver.find_element(By.CSS_SELECTOR, \"div[data-test-id='navigation-bar']\")\n",
    "head_nav_aTag = driver.find_elements(By.CSS_SELECTOR, \"a[data-test-id='navigation-bar-link']\")\n",
    "if head_nav_aTag:\n",
    "    for head_nav in head_nav_aTag:\n",
    "        categories_links.append(head_nav.get_attribute('href'))\n",
    "\n",
    "filtered_categories_urls = [url for url in categories_links if \"https://wolt.com/hr/hrv/split/venue/studenac-kralja-zvonimira-t300/items/\" in url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category URL, we attempt to find subcategories.\n",
    "\n",
    "If a category has no subcategories, the category URL will be forwarded to the `scrape_products()` function.\n",
    "\n",
    "If subcategories are present, we extract the URLs of each subcategory and call the `scrape_products()` function for each subcategory.\n",
    "\n",
    "The `scrape_products()` function is defined in a later cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filtered_categories_urls:\n",
    "    for filtered_category_url in filtered_categories_urls:\n",
    "        page_url = filtered_category_url.replace(\"https://wolt.com\", \"\")\n",
    "        a_Tag = driver.find_element(By.XPATH, f'//a[contains(@href, \"{page_url}\")]')\n",
    "        a_Tag_parent = a_Tag.find_element(By.XPATH, \"./ancestor::div[1]\")\n",
    "        a_Tag.click()\n",
    "        time.sleep(1)\n",
    "        temporary = a_Tag_parent.find_element(By.XPATH, \"./ancestor::div[1]\")\n",
    "        if 'a1qapeeb rljt8w0' in temporary.get_attribute('class'):\n",
    "            subpage_divs = a_Tag_parent.find_elements(By.XPATH, \"./following-sibling::div[1]//a[@data-test-id='navigation-bar-link']\")\n",
    "            if subpage_divs:\n",
    "                for subpage_div in subpage_divs:\n",
    "                    subpage_url = subpage_div.get_attribute('href')\n",
    "                    subcategory_name = subpage_div.find_element(By.XPATH, \"./div[@data-test-id='NavigationListItem-title']\")\n",
    "                    # scrape_products(subpage_url, subcategory_name.text, Store_name)\n",
    "            else:\n",
    "                print(\"Error with getting a tags\")\n",
    "        else:\n",
    "            category_name =  a_Tag.find_element(By.XPATH, \"./div[@data-test-id='NavigationListItem-title']\")\n",
    "            # scrape_products(filtered_category_url, category_name.text, Store_name)\n",
    "            print(\"Else\")\n",
    "\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({ block: 'start', inline: 'nearest'});\", a_Tag_parent)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scrape the data from the (sub)categories using the `scrape_products()` function.\n",
    "\n",
    "First, we handle any pop-ups that may appear.\n",
    "\n",
    "Then, we start scraping the data (product name, product price, product image URL) from dynamically loaded products. Since they are dynamically loaded, scrolling is needed in order to load all the products.\n",
    " \n",
    " The script scrolls to the product that is following the last scraped product to ensure all products are loaded and their data is captured, and it continues to scroll until all products of the page are scraped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all products have been scraped, the data is stored in a CSV file. This CSV file will later be used for analysis.\n",
    "\n",
    "The data is appended to the CSV file if it already exists, ensuring that all scraped data from different categories and stores is consolidated into a single file. This approach allows for easy data manipulation and analysis in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_products(url, cat_name, Store_Name):\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    product_list = []\n",
    "    previous_length = 0\n",
    "    visited_pages = set()\n",
    "\n",
    "    print(f'Wep page: {url}')\n",
    "    try:\n",
    "        consent_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.deorxlo button.cbc_TextButton_rootCss_7cfd4\"))\n",
    "        )\n",
    "        consent_button.click()\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Consent button not found or not clickable: {e}\")\n",
    "\n",
    "    while True:\n",
    "        initial_html = driver.page_source\n",
    "        names_h3 = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='ImageCentricProductCard.Title']\")\n",
    "        seen_products = set([product[0] for product in product_list])\n",
    "        for name_div in names_h3:\n",
    "            name = name_div.text.strip()\n",
    "            if name not in seen_products:\n",
    "                try:\n",
    "                    price_div = driver.find_element(By.XPATH, f'//h3[contains(normalize-space(text()), \"{name.replace(\"'\", \"\\'\")}\")]/../preceding-sibling::div[1]/span')\n",
    "                    price = price_div.text.strip()\n",
    "                    if price_div:\n",
    "                        image_url = price_div.find_element(By.XPATH, \"./../../preceding-sibling::div[1]/span/img\").get_attribute('src')\n",
    "                    else:\n",
    "                        image_url = \"\"\n",
    "                except Exception as e:\n",
    "                    price = \"Price not found\"\n",
    "                product_list.append((name, price, image_url))\n",
    "                seen_products.add(name)\n",
    "                \n",
    "\n",
    "        \n",
    "        # Find and scroll to the last product element\n",
    "        if product_list:\n",
    "            last_product_name = product_list[-1][0].strip().replace(\"'\", \"\\'\")\n",
    "            element = driver.find_element(By.XPATH, f'//h3[contains(normalize-space(text()), \"{last_product_name}\")]')\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({ block: 'start', inline: 'nearest'});\", element)\n",
    "            time.sleep(1.5)\n",
    "        else:\n",
    "            print(\"No products found.\")\n",
    "            break\n",
    "\n",
    "        # Check if new products were loaded\n",
    "        if len(product_list) == previous_length:\n",
    "            break\n",
    "        previous_length = len(product_list)\n",
    "\n",
    "    \n",
    "    print(f'Number of products: {len(product_list)}')\n",
    "    print(product_list)\n",
    "        \n",
    "    driver.quit()\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Create a pandas DataFrame from the product list\n",
    "    df = pd.DataFrame(product_list, columns=['Product Name', 'Price', 'ImageURL']) \n",
    "    df['PageURL'] = url\n",
    "    df['Category'] = cat_name\n",
    "    df['Store'] = Store_Name\n",
    "    df = df[['Product Name', 'Price', 'Category', 'Store', 'ImageURL', 'PageURL']]\n",
    "\n",
    "    # Save the DataFrame to a CSV file, appending if the file exists\n",
    "    file_exists = os.path.isfile('products.csv')\n",
    "    df.to_csv('products.csv', mode='a', header=not file_exists, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created this code by looking at HTML code of the website. We simply tell Selenium to find the tags that contain the data we need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully scraping the data we created a CSV file containing the data from all the stores.\n",
    "\n",
    "**ODE IDE LINK NA CSV FILE I OSTATAK ANALIZE I ZAKLJUCAK**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
